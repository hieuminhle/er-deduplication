{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hash_blocking_spark as hash\n",
    "import ngram_blocking_spark as ngram\n",
    "import matchers_spark as match\n",
    "import similarity_spark as sim\n",
    "import blocking_structured_and_sorted_spark as ss\n",
    "import cluster_spark as cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 32.16172432899475 seconds. Number of index combinations: 1052\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# this was in our experiment in two the best match\n",
    "a = hash.initial_hash_parallel_df(spark,acm_df, selected_columns)\n",
    "b = hash.initial_hash_parallel_df(spark, dblp_df, selected_columns)\n",
    "c = match.apply_similarity_blocks_spark(a,b, 0.7, sim.jaccard_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_index_pairs_to_csv(data, filename):\n",
    "    header = ['dblp_index', 'acm_index']\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)  # Write the header\n",
    "        for row in data:\n",
    "            cleaned_row = [row[0].strip('[]'), row[1].strip('[]')]  # Remove brackets\n",
    "            writer.writerow(cleaned_row)\n",
    "\n",
    "write_index_pairs_to_csv(c, '../Matched/Matched_spark.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matched entites as a list back for spark and without\n",
    "import pandas as pd\n",
    "def reconstructed_pairs(path):\n",
    "    df_pairs = pd.read_csv(path)\n",
    "    return list(zip(df_pairs['dblp_index'], df_pairs['acm_index']))\n",
    "\n",
    "pairs = reconstructed_pairs('../Matched/Matched Entities.csv')\n",
    "pairs_spark = reconstructed_pairs('../Matched/Matched_spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if they are equal\n",
    "def pairs_correspond(pair_list1, pair_list2):\n",
    "    sorted_pair_list1 = [tuple(sorted(pair)) for pair in pair_list1]\n",
    "    sorted_pair_list2 = [tuple(sorted(pair)) for pair in pair_list2]\n",
    "\n",
    "    for pair1 in sorted_pair_list1:\n",
    "        if pair1 not in sorted_pair_list2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "pairs_correspond(pairs_spark, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m cluster_pairs \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mbuild_clusters_parallel(pairs_spark)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mcluster_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cluster_pairs))\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mcluster_to_csv\u001b[1;34m(cluster_data, filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcluster_to_csv\u001b[39m(cluster_data, filename):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m      5\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvfile)\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cluster_data, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# cluster the spark pairs which are equals\n",
    "def cluster_to_csv(cluster_data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for idx, item in enumerate(cluster_data, start=1):\n",
    "            writer.writerow([f'Cluster {idx}: {item}'])\n",
    "\n",
    "cluster_pairs = cluster.build_clusters_parallel(pairs_spark)\n",
    "cluster_to_csv(cluster_pairs, '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv')\n",
    "print(len(cluster_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocking paper_title with 1 augmentations.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"c:\\ProgramData\\anaconda3\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"c:\\ProgramData\\anaconda3\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     dblp_result_df \u001b[38;5;241m=\u001b[39m dblp_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper_title\u001b[39m\u001b[38;5;124m\"\u001b[39m, comb[j](col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper_title\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     33\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhash\u001b[39m\u001b[38;5;241m.\u001b[39minitial_hash_parallel_df(spark,acm_df, selected_columns)\n\u001b[1;32m---> 34\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_hash_parallel_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdblp_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m e \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mapply_similarity_blocks_spark(c,d, \u001b[38;5;241m0.7\u001b[39m, sim\u001b[38;5;241m.\u001b[39mjaccard_similarity)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlocking paper_title with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(com\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mcom\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcomb)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m resulted in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m matches.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sechs\\Documents\\Projects\\DIA-ER\\spark\\hash_blocking_spark.py:60\u001b[0m, in \u001b[0;36minitial_hash_parallel_df\u001b[1;34m(spark, df, columns_to_use)\u001b[0m\n\u001b[0;32m     56\u001b[0m blocks \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocking_key\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mcollect_list(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     58\u001b[0m blocks \u001b[38;5;241m=\u001b[39m blocks\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmapPartitions(hash_partition)\n\u001b[1;32m---> 60\u001b[0m blocks_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[43mblocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     61\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     62\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocking_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     63\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     64\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     65\u001b[0m ])\n\u001b[0;32m     67\u001b[0m data_tuples \u001b[38;5;241m=\u001b[39m [(blocking_key, value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m], value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m blocking_key, value \u001b[38;5;129;01min\u001b[39;00m blocks_dict\u001b[38;5;241m.\u001b[39mitems()]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lower, reverse, col\n",
    "from pyspark.sql.types import StringType\n",
    "from itertools import combinations\n",
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "selected_columns = ['paper_title']\n",
    "\n",
    "@udf(StringType())\n",
    "def alphanumerical(entity: str) -> str:\n",
    "    return ''.join([char for char in entity if char.isalnum()])\n",
    "@udf(StringType())\n",
    "def title(entity: str) -> str:\n",
    "    return entity.title()\n",
    "@udf(StringType())\n",
    "def identity(entity: str) -> str:\n",
    "    return entity\n",
    "\n",
    "augementations = [identity, lower, title, alphanumerical, reverse]\n",
    "for i in range(1, len(augementations)):\n",
    "    print(f\"Blocking paper_title with {i} augmentations.\\n\")\n",
    "    for comb in combinations(augementations, i):\n",
    "        spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "        acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "        dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "        for j in range(len(comb)):\n",
    "            acm_result_df = acm_df.withColumn(\"paper_title\", comb[j](col(\"paper_title\")))\n",
    "            dblp_result_df = dblp_df.withColumn(\"paper_title\", comb[j](col(\"paper_title\")))\n",
    "        if i == 1:\n",
    "            print(acm_result_df.show(1))\n",
    "        c = hash.initial_hash_parallel_df(spark,acm_df, selected_columns)\n",
    "        d = hash.initial_hash_parallel_df(spark, dblp_df, selected_columns)\n",
    "        e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "        print(f\"Blocking paper_title with {' '.join(com.__name__ for com in comb)} resulted in {len(e)} matches.\")\n",
    "        spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 32.73064851760864 seconds. Number of index combinations: 782\n"
     ]
    }
   ],
   "source": [
    "# Example that other are functioning\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = ngram.initial_ngram_parallel_df(spark,acm_df, selected_columns,2)\n",
    "d = ngram.initial_ngram_parallel_df(spark, dblp_df, selected_columns,2)\n",
    "e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
