{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blocking_hash as hash \n",
    "import blocking_ngram as ngram\n",
    "import blocking_structured_and_sort as ss\n",
    "import blocking_length as vl\n",
    "import matchers as m\n",
    "import similarity as sim\n",
    "import cluster as c\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_pairs_to_csv(similar_pairs, output_csv_file):\n",
    "    header = ['dblp_index', 'acm_index']\n",
    "    with open(output_csv_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(header)\n",
    "        for pair in similar_pairs:\n",
    "            writer.writerow(pair)\n",
    "\n",
    "def evaluate_similarity(baseline, comparison):\n",
    "    baseline_set, comparison_set = set(baseline), set(comparison)\n",
    "\n",
    "    tp = len(baseline_set.intersection(comparison_set))\n",
    "    fp = len(comparison_set - baseline_set)\n",
    "    fn = len(baseline_set - comparison_set)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f_measure = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    result = {'precision': precision, 'recall': recall, 'f_measure': f_measure}\n",
    "\n",
    "    return str(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Baselines we make a row wise comparison where we compare respective columns\n",
    "The idea is also to not use id, because both datasets have different ids even for corresponding enteties (no need to run saved as csv (instead run reconstructed method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines 0.7, 0.85 \n",
    "\n",
    "dblp['year'] = dblp['year'].astype(str)\n",
    "acm['year'] = acm['year'].astype(str)\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_7_jac = m.apply_similarity_baseline(dblp, acm, 0.7, selected_columns, sim.jaccard_similarity)\n",
    "similar_pairs_to_csv(base_7_jac,'../baselines/base_7_jac_stem.csv')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_85_jac = m.apply_similarity_baseline(dblp, acm, 0.85, selected_columns, sim.jaccard_similarity)\n",
    "similar_pairs_to_csv(base_85_jac,'../baselines/base_85_jac_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many pairs therefore we dont try to compare with a blocking tehcnique cause they would only achieve few thousands\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_7_n = m.apply_similarity_baseline(dblp, acm, 0.7, selected_columns, sim.n_gram_similarity)\n",
    "similar_pairs_to_csv(base_7_n,'../baselines/base_7_n_stem.csv')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_85_n = m.apply_similarity_baseline(dblp, acm, 0.85, selected_columns, sim.n_gram_similarity)\n",
    "similar_pairs_to_csv(base_85_n,'../baselines/base_85_n_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_7_n = m.apply_similarity_baseline(dblp, acm, 0.7, selected_columns, sim.exact_length_similarity)\n",
    "similar_pairs_to_csv(base_7_n,'../baselines/base_7_l_stem.csv')\n",
    "\n",
    "# 0 pairs therefore not used\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_85_n = m.apply_similarity_baseline(dblp, acm, 0.85, selected_columns, sim.exact_length_similarity)\n",
    "similar_pairs_to_csv(base_85_n,'../baselines/base_85_l_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_7_lev = m.apply_similarity_baseline(dblp, acm, 0.7, selected_columns, sim.levensthein_distance)\n",
    "similar_pairs_to_csv(base_7_lev,'../baselines/base_7_lev_stem.csv')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_85_lev = m.apply_similarity_baseline(dblp, acm, 0.85, selected_columns, sim.levensthein_distance)\n",
    "similar_pairs_to_csv(base_85_lev,'../baselines/base_85_lev_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the baselines take a time to compute so to not compute it again save csv and transform to list pairs\n",
    "def reconstructed_pairs(path):\n",
    "    df_pairs = pd.read_csv(path)\n",
    "    return list(zip(df_pairs['dblp_index'], df_pairs['acm_index']))\n",
    "\n",
    "base_7_jac = reconstructed_pairs('../baselines/base_7_jac_stem.csv')\n",
    "base_85_jac = reconstructed_pairs('../baselines/base_85_jac_stem.csv')\n",
    "\n",
    "base_7_l = reconstructed_pairs('../baselines/base_7_l_stem.csv')\n",
    "base_85_l = reconstructed_pairs('../baselines/base_85_l_stem.csv')\n",
    "\n",
    "base_7_n = reconstructed_pairs('../baselines/base_7_n_stem.csv')\n",
    "base_85_n = reconstructed_pairs('../baselines/base_85_n_stem.csv')\n",
    "\n",
    "base_7_lev = reconstructed_pairs('../baselines/base_7_lev_stem.csv')\n",
    "base_85_lev = reconstructed_pairs('../baselines/base_85_lev_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when using sorted bloking based on year or year & publication_venue we can block created blocks for example with ngram or hash\n",
    "# therefore we would transform blocks to dataframes witg blocks_to_df and with blocks_df apply on all the df the blocking function like ngram\n",
    "\n",
    "def blocks_to_df(blocks):\n",
    "    dfs = []\n",
    "    for block in blocks:\n",
    "        df_block = pd.DataFrame(block)\n",
    "        dfs.append(df_block)\n",
    "    return dfs\n",
    "\n",
    "def block_dfs(dataframes, blocking_function, *args):\n",
    "    blocks = {}\n",
    "    for df in dataframes:\n",
    "        block = blocking_function(df, *args)\n",
    "        blocks.update(block)\n",
    "    return blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are many possible matches, but sorted blocking with additional blocking like ngram will not be used when columns year and publisher are used, because this would lead to more matches due to sorted each block compared block has the same year and publisher -> influences match negatively more pairs\n",
    "\n",
    "example it sorts trough years lets say 1995 but then for 1995 we would have two blocks one with vldb and another with sigmod as venue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 7.1769859790802 seconds. Number of similar pairs: 2495\n",
      "Processing time: 4.740327835083008 seconds. Number of similar pairs: 90801\n",
      "Processing time: 8.75877594947815 seconds. Number of similar pairs: 2118\n",
      "Processing time: 9.542505025863647 seconds. Number of similar pairs: 2118\n",
      "Processing time: 3.751330614089966 seconds. Number of similar pairs: 2480\n",
      "Processing time: 3.907388210296631 seconds. Number of similar pairs: 2216\n",
      "Processing time: 0.026314973831176758 seconds. Number of similar pairs: 50143\n",
      "Processing time: 0.8830711841583252 seconds. Number of similar pairs: 19261\n",
      "Processing time: 0.09289216995239258 seconds. Number of similar pairs: 221\n",
      "Processing time: 0.03512310981750488 seconds. Number of similar pairs: 29\n",
      "Processing time: 0.795835018157959 seconds. Number of similar pairs: 70890\n",
      "Processing time: 0.767827033996582 seconds. Number of similar pairs: 69936\n",
      "Processing time: 5.524066925048828 seconds. Number of similar pairs: 1056\n",
      "Processing time: 5.541990280151367 seconds. Number of similar pairs: 2096\n",
      "Processing time: 10.73028016090393 seconds. Number of similar pairs: 1020\n",
      "Processing time: 10.932745933532715 seconds. Number of similar pairs: 1020\n",
      "Processing time: 4.701664924621582 seconds. Number of similar pairs: 1052\n",
      "Processing time: 4.651182651519775 seconds. Number of similar pairs: 1090\n",
      "Processing time: 0.016357898712158203 seconds. Number of similar pairs: 60887\n",
      "Processing time: 0.8284878730773926 seconds. Number of similar pairs: 50863\n",
      "Processing time: 0.06137514114379883 seconds. Number of similar pairs: 468\n",
      "Processing time: 0.055265188217163086 seconds. Number of similar pairs: 74\n",
      "Processing time: 0.975348949432373 seconds. Number of similar pairs: 77546\n",
      "Processing time: 0.8611958026885986 seconds. Number of similar pairs: 77321\n",
      "Processing time: 6.407607078552246 seconds. Number of similar pairs: 782\n",
      "Processing time: 6.263146877288818 seconds. Number of similar pairs: 782\n",
      "Processing time: 15.380800008773804 seconds. Number of similar pairs: 0\n",
      "Processing time: 17.31899905204773 seconds. Number of similar pairs: 0\n",
      "Processing time: 4.75147008895874 seconds. Number of similar pairs: 782\n",
      "Processing time: 4.76875114440918 seconds. Number of similar pairs: 706\n",
      "Processing time: 2.021428108215332 seconds. Number of similar pairs: 1588\n",
      "Processing time: 1.7549591064453125 seconds. Number of similar pairs: 15378\n",
      "Processing time: 0.23162293434143066 seconds. Number of similar pairs: 1285\n",
      "Processing time: 0.24618983268737793 seconds. Number of similar pairs: 222\n",
      "Processing time: 0.9016997814178467 seconds. Number of similar pairs: 78147\n",
      "Processing time: 0.8645346164703369 seconds. Number of similar pairs: 78631\n",
      "Processing time: 6.841203927993774 seconds. Number of similar pairs: 765\n",
      "Processing time: 6.794358968734741 seconds. Number of similar pairs: 765\n",
      "Processing time: 17.86402988433838 seconds. Number of similar pairs: 0\n",
      "Processing time: 18.886619329452515 seconds. Number of similar pairs: 0\n",
      "Processing time: 4.954296827316284 seconds. Number of similar pairs: 765\n",
      "Processing time: 4.917178153991699 seconds. Number of similar pairs: 684\n",
      "Processing time: 2.0100343227386475 seconds. Number of similar pairs: 1588\n",
      "Processing time: 6.8818418979644775 seconds. Number of similar pairs: 781\n",
      "Processing time: 6.616969108581543 seconds. Number of similar pairs: 781\n",
      "Processing time: 18.059895038604736 seconds. Number of similar pairs: 0\n",
      "Processing time: 18.903064727783203 seconds. Number of similar pairs: 0\n",
      "Processing time: 4.810106992721558 seconds. Number of similar pairs: 781\n",
      "Processing time: 4.758818864822388 seconds. Number of similar pairs: 705\n",
      "Processing time: 2.4196460247039795 seconds. Number of similar pairs: 1203\n",
      "Processing time: 7.316386699676514 seconds. Number of similar pairs: 765\n",
      "Processing time: 7.152241945266724 seconds. Number of similar pairs: 765\n",
      "Processing time: 19.88222098350525 seconds. Number of similar pairs: 0\n",
      "Processing time: 20.54868197441101 seconds. Number of similar pairs: 0\n",
      "Processing time: 4.904161691665649 seconds. Number of similar pairs: 765\n",
      "Processing time: 4.951894998550415 seconds. Number of similar pairs: 684\n",
      "Processing time: 2.4522640705108643 seconds. Number of similar pairs: 1203\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "year_block = [1995,1996,1997, 1998, 1999,2000,2001, 2002, 2003, 2004,2005]\n",
    "labels = [\"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\"]\n",
    "\n",
    "hash_indices = ['hash_value']\n",
    "ngram_indices = ['ngram_values']\n",
    "\n",
    "\n",
    "selected_columns = ['author_names']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_a_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_a_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_a_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_a_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_a_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_a_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_a = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "# sorted blocking by year and publisher\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_a = m.apply_similarity_sorted(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)\n",
    "\n",
    "# sorted blocking by year and publisher with initial_ngram = 2, 3, initial hash and hash blocking \n",
    "dblp_sn2 = blocks_to_df(dblp_s)\n",
    "dblp_sn2 = block_dfs(dblp_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "acm_sn2 = blocks_to_df(acm_s)\n",
    "acm_sn2 = block_dfs(acm_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "sorted_initial_n2_a = m.apply_similarity_sorted_dictionary(dblp_sn2, acm_sn2, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_sn3 = blocks_to_df(dblp_s)\n",
    "dblp_sn3 = block_dfs(dblp_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "acm_sn3 = blocks_to_df(acm_s)\n",
    "acm_sn3 = block_dfs(acm_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "sorted_initial_n3_a = m.apply_similarity_sorted_dictionary(dblp_sn3, acm_sn3, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.hash_blocking, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.hash_blocking , selected_columns)\n",
    "sorted_h_a = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.initial_hash, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.initial_hash , selected_columns)\n",
    "sorted_initial_h_a = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_p_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_p_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_p_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_p_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_p_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_p_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_p = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "# sorted blocking by year and publisher\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_p = m.apply_similarity_sorted(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)\n",
    "\n",
    "# sorted blocking by year and publisher with initial_ngram = 2, 3, initial hash and hash blocking \n",
    "dblp_sn2 = blocks_to_df(dblp_s)\n",
    "dblp_sn2 = block_dfs(dblp_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "acm_sn2 = blocks_to_df(acm_s)\n",
    "acm_sn2 = block_dfs(acm_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "sorted_initial_n2_p = m.apply_similarity_sorted_dictionary(dblp_sn2, acm_sn2, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_sn3 = blocks_to_df(dblp_s)\n",
    "dblp_sn3 = block_dfs(dblp_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "acm_sn3 = blocks_to_df(acm_s)\n",
    "acm_sn3 = block_dfs(acm_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "sorted_initial_n3_p = m.apply_similarity_sorted_dictionary(dblp_sn3, acm_sn3, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.hash_blocking, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.hash_blocking , selected_columns)\n",
    "sorted_h_p = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.initial_hash, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.initial_hash , selected_columns)\n",
    "sorted_initial_h_p = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_ap_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_ap_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_ap_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_ap_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_ap_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_ap_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_ap = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "# sorted blocking by year and publisher\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_ap = m.apply_similarity_sorted(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)\n",
    "\n",
    "# sorted blocking by year and publisher with initial_ngram = 2, 3, initial hash and hash blocking \n",
    "dblp_sn2 = blocks_to_df(dblp_s)\n",
    "dblp_sn2 = block_dfs(dblp_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "acm_sn2 = blocks_to_df(acm_s)\n",
    "acm_sn2 = block_dfs(acm_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "sorted_initial_n2_ap = m.apply_similarity_sorted_dictionary(dblp_sn2, acm_sn2, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_sn3 = blocks_to_df(dblp_s)\n",
    "dblp_sn3 = block_dfs(dblp_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "acm_sn3 = blocks_to_df(acm_s)\n",
    "acm_sn3 = block_dfs(acm_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "sorted_initial_n3_ap = m.apply_similarity_sorted_dictionary(dblp_sn3, acm_sn3, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.hash_blocking, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.hash_blocking , selected_columns)\n",
    "sorted_h_ap = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.initial_hash, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.initial_hash , selected_columns)\n",
    "sorted_initial_h_ap = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'year']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_apy_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_apy_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_apy_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_apy_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_apy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_apy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_apy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'publication_venue']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_appv_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_appv_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_appv_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_appv_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_appv_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_appv_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_appv = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'publication_venue', 'year']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_appvy_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_appvy_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_appvy_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_appvy_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_appvy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_appvy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_appvy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3438877755511022, 'recall': 0.7951807228915663, 'f_measure': 0.4801343033016229}\n",
      "{'precision': 0.009449235140582152, 'recall': 0.7951807228915663, 'f_measure': 0.01867653461036134}\n",
      "{'precision': 0.37063267233238906, 'recall': 0.7275254865616312, 'f_measure': 0.4910853925555208}\n",
      "{'precision': 0.37063267233238906, 'recall': 0.7275254865616312, 'f_measure': 0.4910853925555208}\n",
      "{'precision': 0.9005681818181818, 'recall': 0.881371640407785, 'f_measure': 0.8908665105386417}\n",
      "{'precision': 0.45372137404580154, 'recall': 0.881371640407785, 'f_measure': 0.5990551181102363}\n",
      "{'precision': 0.9137254901960784, 'recall': 0.8637627432808156, 'f_measure': 0.88804192472606}\n",
      "{'precision': 0.9137254901960784, 'recall': 0.8637627432808156, 'f_measure': 0.88804192472606}\n",
      "{'precision': 0.9757033248081841, 'recall': 0.7071362372567191, 'f_measure': 0.8199892530897367}\n",
      "{'precision': 0.9757033248081841, 'recall': 0.7071362372567191, 'f_measure': 0.8199892530897367}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.9769526248399488, 'recall': 0.7071362372567191, 'f_measure': 0.8204301075268817}\n",
      "{'precision': 0.9769526248399488, 'recall': 0.7071362372567191, 'f_measure': 0.8204301075268817}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n"
     ]
    }
   ],
   "source": [
    "# evaluate all ngram variants\n",
    "\n",
    "result_combined_ngram_07 = (\n",
    "    evaluate_similarity(base_7_jac, initial_n2_a_07_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n3_a_07_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n2_a_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n3_a_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n2_p_07_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n3_p_07_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n2_p_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n3_p_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n2_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n3_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n2_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n3_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n2_apy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n3_apy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n2_apy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n3_apy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n2_appv_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n3_appv_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n2_appv_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n3_appv_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n2_appvy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n3_appvy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n2_appvy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, n3_appvy_07_jac)\n",
    ")\n",
    "\n",
    "print(result_combined_ngram_07)\n",
    "\n",
    "# initial_n2_p_07_jac and all ap variants\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.34596774193548385, 'recall': 0.7951807228915663, 'f_measure': 0.48215790952514753}\n",
      "{'precision': 0.37063267233238906, 'recall': 0.7275254865616312, 'f_measure': 0.4910853925555208}\n",
      "{'precision': 0.9039923954372624, 'recall': 0.881371640407785, 'f_measure': 0.8925387142186767}\n",
      "{'precision': 0.9137254901960784, 'recall': 0.8637627432808156, 'f_measure': 0.88804192472606}\n",
      "{'precision': 0.9757033248081841, 'recall': 0.7071362372567191, 'f_measure': 0.8199892530897367}\n",
      "{'precision': 0.9785407725321889, 'recall': 0.633920296570899, 'f_measure': 0.7694038245219348}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 1.0, 'recall': 0.6320667284522706, 'f_measure': 0.7745599091425327}\n",
      "{'precision': 0.9769526248399488, 'recall': 0.7071362372567191, 'f_measure': 0.8204301075268817}\n",
      "{'precision': 0.9799426934097422, 'recall': 0.633920296570899, 'f_measure': 0.7698368036015757}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 1.0, 'recall': 0.6320667284522706, 'f_measure': 0.7745599091425327}\n"
     ]
    }
   ],
   "source": [
    "result_combined_hash_07 = (\n",
    "    evaluate_similarity(base_7_jac, initial_h_a_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, h_a_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_h_p_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, h_p_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_h_ap_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, h_ap_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_h_apy_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, h_apy_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_h_appv_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, h_appv_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_h_appvy_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, h_appvy_jac)\n",
    "    \n",
    ")\n",
    "\n",
    "print(result_combined_hash_07)\n",
    "\n",
    "# initial h_p_jac (match), h_p_jac and initial_h_ap_jac are the best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.016834258330090723, 'recall': 0.7618164967562558, 'f_measure': 0.03294061072373166}\n",
      "{'precision': 0.015784401688100223, 'recall': 0.8804448563484708, 'f_measure': 0.03101281318860687}\n",
      "{'precision': 0.4600253807106599, 'recall': 0.6719184430027804, 'f_measure': 0.5461393596986818}\n",
      "{'precision': 0.4600253807106599, 'recall': 0.6719184430027804, 'f_measure': 0.5461393596986818}\n",
      "{'precision': 0.606694560669456, 'recall': 0.6719184430027804, 'f_measure': 0.6376429199648197}\n",
      "{'precision': 0.606694560669456, 'recall': 0.6719184430027804, 'f_measure': 0.6376429199648197}\n"
     ]
    }
   ],
   "source": [
    "result_combined_length_07 = (\n",
    "    evaluate_similarity(base_7_jac, length_a)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, length_p)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, length_ap)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, length_apy)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, length_appv)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, length_appvy)      \n",
    ")\n",
    "\n",
    "print(result_combined_length_07)\n",
    "\n",
    "# length_appv and length_appvy are ok - good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.055831460086779236, 'recall': 0.989805375347544, 'f_measure': 0.10570071258907363}\n",
      "{'precision': 0.006535947712418301, 'recall': 0.0009267840593141798, 'f_measure': 0.0016233766233766235}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0009208103130755065, 'recall': 0.0009267840593141798, 'f_measure': 0.0009237875288683603}\n",
      "{'precision': 0.020760355439450612, 'recall': 0.9721964782205746, 'f_measure': 0.040652611998139826}\n",
      "{'precision': 0.006493506493506494, 'recall': 0.0018535681186283596, 'f_measure': 0.0028839221341023795}\n",
      "{'precision': 0.03389830508474576, 'recall': 0.0018535681186283596, 'f_measure': 0.00351493848857645}\n",
      "{'precision': 0.0008264462809917355, 'recall': 0.0009267840593141798, 'f_measure': 0.0008737439930100481}\n",
      "{'precision': 0.001658374792703151, 'recall': 0.0018535681186283596, 'f_measure': 0.00175054704595186}\n",
      "{'precision': 0.07020413504318242, 'recall': 0.9944392956441149, 'f_measure': 0.13114954470451629}\n",
      "{'precision': 0.004754358161648178, 'recall': 0.0027803521779425394, 'f_measure': 0.0035087719298245615}\n",
      "{'precision': 0.02142857142857143, 'recall': 0.0027803521779425394, 'f_measure': 0.004922067268252666}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0024509803921568627, 'recall': 0.0027803521779425394, 'f_measure': 0.002605297438124186}\n"
     ]
    }
   ],
   "source": [
    "result_combined_sorted_07 = (\n",
    "    evaluate_similarity(base_7_jac, sorted_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_n2_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_n3_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_h_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_h_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_n2_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_n3_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_h_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_h_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_n2_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_n3_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_h_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_initial_h_ap) \n",
    ")\n",
    "\n",
    "print(result_combined_sorted_07)\n",
    "\n",
    "# all bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 5.074635744094849 seconds. Number of similar pairs: 1052\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_to_csv(initial_h_p_jac,'../Matched/Matched Entities.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 7.127413988113403 seconds. Number of similar pairs: 2495\n",
      "Processing time: 4.592610836029053 seconds. Number of similar pairs: 90801\n",
      "Processing time: 11.911108016967773 seconds. Number of similar pairs: 2118\n",
      "Processing time: 8.95640516281128 seconds. Number of similar pairs: 2118\n",
      "Processing time: 3.7122960090637207 seconds. Number of similar pairs: 2480\n",
      "Processing time: 3.9715867042541504 seconds. Number of similar pairs: 2216\n",
      "Processing time: 0.02616119384765625 seconds. Number of similar pairs: 50143\n",
      "Processing time: 0.882011890411377 seconds. Number of similar pairs: 1877\n",
      "Processing time: 0.04324603080749512 seconds. Number of similar pairs: 221\n",
      "Processing time: 0.035524845123291016 seconds. Number of similar pairs: 29\n",
      "Processing time: 0.7886888980865479 seconds. Number of similar pairs: 70890\n",
      "Processing time: 0.7748508453369141 seconds. Number of similar pairs: 69936\n",
      "Processing time: 5.499645948410034 seconds. Number of similar pairs: 1056\n",
      "Processing time: 5.398442029953003 seconds. Number of similar pairs: 2096\n",
      "Processing time: 10.352702856063843 seconds. Number of similar pairs: 1020\n",
      "Processing time: 10.974231958389282 seconds. Number of similar pairs: 1020\n",
      "Processing time: 4.616927146911621 seconds. Number of similar pairs: 1052\n",
      "Processing time: 4.762571096420288 seconds. Number of similar pairs: 1090\n",
      "Processing time: 0.021558046340942383 seconds. Number of similar pairs: 60887\n",
      "Processing time: 1.2711882591247559 seconds. Number of similar pairs: 4748\n",
      "Processing time: 0.07015395164489746 seconds. Number of similar pairs: 468\n",
      "Processing time: 0.05777573585510254 seconds. Number of similar pairs: 74\n",
      "Processing time: 1.1445329189300537 seconds. Number of similar pairs: 77546\n",
      "Processing time: 0.9728319644927979 seconds. Number of similar pairs: 77321\n",
      "Processing time: 8.148896932601929 seconds. Number of similar pairs: 782\n",
      "Processing time: 9.96883487701416 seconds. Number of similar pairs: 782\n",
      "Processing time: 19.451857089996338 seconds. Number of similar pairs: 0\n",
      "Processing time: 21.58579421043396 seconds. Number of similar pairs: 0\n",
      "Processing time: 7.866601943969727 seconds. Number of similar pairs: 782\n",
      "Processing time: 5.997622966766357 seconds. Number of similar pairs: 706\n",
      "Processing time: 3.1284968852996826 seconds. Number of similar pairs: 1588\n",
      "Processing time: 2.8895621299743652 seconds. Number of similar pairs: 1133\n",
      "Processing time: 0.2509188652038574 seconds. Number of similar pairs: 1285\n",
      "Processing time: 0.2492530345916748 seconds. Number of similar pairs: 222\n",
      "Processing time: 1.1889541149139404 seconds. Number of similar pairs: 78147\n",
      "Processing time: 0.9559309482574463 seconds. Number of similar pairs: 78631\n",
      "Processing time: 8.32529616355896 seconds. Number of similar pairs: 765\n",
      "Processing time: 8.669932126998901 seconds. Number of similar pairs: 765\n",
      "Processing time: 20.9851131439209 seconds. Number of similar pairs: 0\n",
      "Processing time: 19.104785919189453 seconds. Number of similar pairs: 0\n",
      "Processing time: 5.0823609828948975 seconds. Number of similar pairs: 765\n",
      "Processing time: 4.992253065109253 seconds. Number of similar pairs: 684\n",
      "Processing time: 2.074504852294922 seconds. Number of similar pairs: 1588\n",
      "Processing time: 6.83540678024292 seconds. Number of similar pairs: 781\n",
      "Processing time: 6.784841060638428 seconds. Number of similar pairs: 781\n",
      "Processing time: 17.143861055374146 seconds. Number of similar pairs: 0\n",
      "Processing time: 18.77948498725891 seconds. Number of similar pairs: 0\n",
      "Processing time: 4.843489170074463 seconds. Number of similar pairs: 781\n",
      "Processing time: 4.900173902511597 seconds. Number of similar pairs: 705\n",
      "Processing time: 2.514723777770996 seconds. Number of similar pairs: 1203\n",
      "Processing time: 7.063024997711182 seconds. Number of similar pairs: 765\n",
      "Processing time: 6.93594217300415 seconds. Number of similar pairs: 765\n",
      "Processing time: 19.131248950958252 seconds. Number of similar pairs: 0\n",
      "Processing time: 20.07723379135132 seconds. Number of similar pairs: 0\n",
      "Processing time: 4.9725611209869385 seconds. Number of similar pairs: 765\n",
      "Processing time: 5.030915975570679 seconds. Number of similar pairs: 684\n",
      "Processing time: 2.5209238529205322 seconds. Number of similar pairs: 1203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "threshold = 0.85\n",
    "\n",
    "year_block = [1995,1996,1997, 1998, 1999,2000,2001, 2002, 2003, 2004,2005]\n",
    "labels = [\"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\"]\n",
    "\n",
    "hash_indices = ['hash_value']\n",
    "ngram_indices = ['ngram_values']\n",
    "\n",
    "\n",
    "selected_columns = ['author_names']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_a_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_a_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_a_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_a_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_a_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_a_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_a = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "# sorted blocking by year and publisher\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_a = m.apply_similarity_sorted(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)\n",
    "\n",
    "# sorted blocking by year and publisher with initial_ngram = 2, 3, initial hash and hash blocking \n",
    "dblp_sn2 = blocks_to_df(dblp_s)\n",
    "dblp_sn2 = block_dfs(dblp_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "acm_sn2 = blocks_to_df(acm_s)\n",
    "acm_sn2 = block_dfs(acm_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "sorted_initial_n2_a = m.apply_similarity_sorted_dictionary(dblp_sn2, acm_sn2, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_sn3 = blocks_to_df(dblp_s)\n",
    "dblp_sn3 = block_dfs(dblp_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "acm_sn3 = blocks_to_df(acm_s)\n",
    "acm_sn3 = block_dfs(acm_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "sorted_initial_n3_a = m.apply_similarity_sorted_dictionary(dblp_sn3, acm_sn3, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.hash_blocking, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.hash_blocking , selected_columns)\n",
    "sorted_h_a = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.initial_hash, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.initial_hash , selected_columns)\n",
    "sorted_initial_h_a = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_p_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_p_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_p_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_p_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_p_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_p_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_p = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "# sorted blocking by year and publisher\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_p = m.apply_similarity_sorted(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)\n",
    "\n",
    "# sorted blocking by year and publisher with initial_ngram = 2, 3, initial hash and hash blocking \n",
    "dblp_sn2 = blocks_to_df(dblp_s)\n",
    "dblp_sn2 = block_dfs(dblp_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "acm_sn2 = blocks_to_df(acm_s)\n",
    "acm_sn2 = block_dfs(acm_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "sorted_initial_n2_p = m.apply_similarity_sorted_dictionary(dblp_sn2, acm_sn2, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_sn3 = blocks_to_df(dblp_s)\n",
    "dblp_sn3 = block_dfs(dblp_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "acm_sn3 = blocks_to_df(acm_s)\n",
    "acm_sn3 = block_dfs(acm_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "sorted_initial_n3_p = m.apply_similarity_sorted_dictionary(dblp_sn3, acm_sn3, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.hash_blocking, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.hash_blocking , selected_columns)\n",
    "sorted_h_p = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.initial_hash, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.initial_hash , selected_columns)\n",
    "sorted_initial_h_p = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_ap_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_ap_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_ap_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_ap_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_ap_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_ap_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_ap = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "# sorted blocking by year and publisher\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_ap = m.apply_similarity_sorted(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)\n",
    "\n",
    "# sorted blocking by year and publisher with initial_ngram = 2, 3, initial hash and hash blocking \n",
    "dblp_sn2 = blocks_to_df(dblp_s)\n",
    "dblp_sn2 = block_dfs(dblp_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "acm_sn2 = blocks_to_df(acm_s)\n",
    "acm_sn2 = block_dfs(acm_sn2, ngram.initial_ngram, 2, selected_columns)\n",
    "sorted_initial_n2_ap = m.apply_similarity_sorted_dictionary(dblp_sn2, acm_sn2, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_sn3 = blocks_to_df(dblp_s)\n",
    "dblp_sn3 = block_dfs(dblp_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "acm_sn3 = blocks_to_df(acm_s)\n",
    "acm_sn3 = block_dfs(acm_sn3, ngram.initial_ngram, 3, selected_columns)\n",
    "sorted_initial_n3_ap = m.apply_similarity_sorted_dictionary(dblp_sn3, acm_sn3, threshold, sim.jaccard_similarity_ngrams, 'ngram_values')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.hash_blocking, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.hash_blocking , selected_columns)\n",
    "sorted_h_ap = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "dblp_h = blocks_to_df(dblp_s)\n",
    "dblp_h = block_dfs(dblp_h, hash.initial_hash, selected_columns)\n",
    "acm_h = blocks_to_df(acm_s)\n",
    "acm_h = block_dfs(acm_h, hash.initial_hash , selected_columns)\n",
    "sorted_initial_h_ap = m.apply_similarity_sorted_dictionary(dblp_h, acm_h, threshold, sim.jaccard_similarity, 'hash_value')\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'year']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_apy_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_apy_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_apy_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_apy_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_apy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_apy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_apy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'publication_venue']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_appv_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_appv_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_appv_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_appv_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_appv_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_appv_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_appv = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'publication_venue', 'year']\n",
    "\n",
    "# initial n-gram with n = 2,3 \n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_appvy_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_appvy_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "# n-gram blocking with n = 2, 3\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_appvy_85_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_appvy_85_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "# initial hash blocking\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_appvy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# hash blocking\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_appvy_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "# length blocking\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_appvy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.jaccard_similarity, 'lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3438877755511022, 'recall': 0.7951807228915663, 'f_measure': 0.4801343033016229}\n",
      "{'precision': 0.009449235140582152, 'recall': 0.7951807228915663, 'f_measure': 0.01867653461036134}\n",
      "{'precision': 0.37063267233238906, 'recall': 0.7275254865616312, 'f_measure': 0.4910853925555208}\n",
      "{'precision': 0.37063267233238906, 'recall': 0.7275254865616312, 'f_measure': 0.4910853925555208}\n",
      "{'precision': 0.9005681818181818, 'recall': 0.881371640407785, 'f_measure': 0.8908665105386417}\n",
      "{'precision': 0.45372137404580154, 'recall': 0.881371640407785, 'f_measure': 0.5990551181102363}\n",
      "{'precision': 0.9137254901960784, 'recall': 0.8637627432808156, 'f_measure': 0.88804192472606}\n",
      "{'precision': 0.9137254901960784, 'recall': 0.8637627432808156, 'f_measure': 0.88804192472606}\n",
      "{'precision': 0.9757033248081841, 'recall': 0.7071362372567191, 'f_measure': 0.8199892530897367}\n",
      "{'precision': 0.9757033248081841, 'recall': 0.7071362372567191, 'f_measure': 0.8199892530897367}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.9769526248399488, 'recall': 0.7071362372567191, 'f_measure': 0.8204301075268817}\n",
      "{'precision': 0.9769526248399488, 'recall': 0.7071362372567191, 'f_measure': 0.8204301075268817}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0, 'recall': 0.0, 'f_measure': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_combined_ngram_85 = (\n",
    "    evaluate_similarity(base_85_jac, initial_n2_a_85_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n3_a_85_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n2_a_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n3_a_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n2_p_85_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n3_p_85_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n2_p_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n3_p_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n2_ap_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n3_ap_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n2_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n3_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n2_apy_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n3_apy_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n2_apy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n3_apy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n2_appv_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n3_appv_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n2_appv_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n3_appv_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n2_appvy_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_n3_appvy_85_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n2_appvy_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, n3_appvy_07_jac)\n",
    ")\n",
    "\n",
    "print(result_combined_ngram_07)\n",
    "\n",
    "# initial_n2_p_85_jac (match), n2_p_07_jac, initial_n2_apy_85_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.34596774193548385, 'recall': 0.7951807228915663, 'f_measure': 0.48215790952514753}\n",
      "{'precision': 0.37063267233238906, 'recall': 0.7275254865616312, 'f_measure': 0.4910853925555208}\n",
      "{'precision': 0.9039923954372624, 'recall': 0.881371640407785, 'f_measure': 0.8925387142186767}\n",
      "{'precision': 0.9137254901960784, 'recall': 0.8637627432808156, 'f_measure': 0.88804192472606}\n",
      "{'precision': 0.9757033248081841, 'recall': 0.7071362372567191, 'f_measure': 0.8199892530897367}\n",
      "{'precision': 0.9785407725321889, 'recall': 0.633920296570899, 'f_measure': 0.7694038245219348}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 1.0, 'recall': 0.6320667284522706, 'f_measure': 0.7745599091425327}\n",
      "{'precision': 0.9769526248399488, 'recall': 0.7071362372567191, 'f_measure': 0.8204301075268817}\n",
      "{'precision': 0.9799426934097422, 'recall': 0.633920296570899, 'f_measure': 0.7698368036015757}\n",
      "{'precision': 0.9947712418300654, 'recall': 0.7052826691380908, 'f_measure': 0.8253796095444687}\n",
      "{'precision': 1.0, 'recall': 0.6320667284522706, 'f_measure': 0.7745599091425327}\n"
     ]
    }
   ],
   "source": [
    "result_combined_hash_85 = (\n",
    "    evaluate_similarity(base_85_jac, initial_h_a_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, h_a_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_h_p_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, h_p_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_h_ap_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, h_ap_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_h_apy_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, h_apy_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_h_appv_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, h_appv_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, initial_h_appvy_jac)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, h_appvy_jac)\n",
    "    \n",
    ")\n",
    "\n",
    "print(result_combined_hash_07)\n",
    "\n",
    "# initial_h_p_jac, h_p_jac, initial_h_apy_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.014294783837473632, 'recall': 0.9136125654450262, 'f_measure': 0.028149133950355894}\n",
      "{'precision': 0.012527830392450071, 'recall': 0.9869109947643979, 'f_measure': 0.024741591468416733}\n",
      "{'precision': 0.4384517766497462, 'recall': 0.9044502617801047, 'f_measure': 0.5905982905982906}\n",
      "{'precision': 0.4384517766497462, 'recall': 0.9044502617801047, 'f_measure': 0.5905982905982906}\n",
      "{'precision': 0.5782426778242677, 'recall': 0.9044502617801047, 'f_measure': 0.7054619703930576}\n",
      "{'precision': 0.5782426778242677, 'recall': 0.9044502617801047, 'f_measure': 0.7054619703930576}\n"
     ]
    }
   ],
   "source": [
    "result_combined_length_85 = (\n",
    "    evaluate_similarity(base_85_jac, length_a)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, length_p)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, length_ap)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, length_apy)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, length_appv)  +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, length_appvy)      \n",
    ")\n",
    "\n",
    "print(result_combined_length_85)\n",
    "\n",
    "# length_app and length_appy okay - good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.41106290672451196, 'recall': 0.9921465968586387, 'f_measure': 0.5812883435582822}\n",
      "{'precision': 0.006535947712418301, 'recall': 0.0013089005235602095, 'f_measure': 0.0021810250817884407}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0009208103130755065, 'recall': 0.0013089005235602095, 'f_measure': 0.0010810810810810813}\n",
      "{'precision': 0.16165890816758358, 'recall': 1.0, 'f_measure': 0.27832422586520944}\n",
      "{'precision': 0.006493506493506494, 'recall': 0.002617801047120419, 'f_measure': 0.0037313432835820895}\n",
      "{'precision': 0.03389830508474576, 'recall': 0.002617801047120419, 'f_measure': 0.004860267314702308}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.001658374792703151, 'recall': 0.002617801047120419, 'f_measure': 0.002030456852791878}\n",
      "{'precision': 0.6773049645390071, 'recall': 1.0, 'f_measure': 0.8076109936575052}\n",
      "{'precision': 0.004754358161648178, 'recall': 0.003926701570680628, 'f_measure': 0.004301075268817204}\n",
      "{'precision': 0.02142857142857143, 'recall': 0.003926701570680628, 'f_measure': 0.00663716814159292}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0024509803921568627, 'recall': 0.003926701570680628, 'f_measure': 0.003018108651911469}\n"
     ]
    }
   ],
   "source": [
    "result_combined_sorted_85 = (\n",
    "    evaluate_similarity(base_85_jac, sorted_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_n2_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_n3_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_h_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_h_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_n2_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_n3_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_h_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_h_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_n2_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_n3_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_h_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_85_jac, sorted_initial_h_ap) \n",
    ")\n",
    "\n",
    "print(result_combined_sorted_85)\n",
    "\n",
    "# only sorted_ap good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs_to_csv(initial_n2_p_85_jac,'../Matched/base_85_jac_match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 0.19475889205932617 seconds. Number of similar pairs: 50143\n",
      "Processing time: 0.010220050811767578 seconds. Number of similar pairs: 60887\n",
      "Processing time: 0.8428208827972412 seconds. Number of similar pairs: 1588\n",
      "Processing time: 0.10789728164672852 seconds. Number of similar pairs: 287\n",
      "Processing time: 0.028612136840820312 seconds. Number of similar pairs: 50143\n",
      "Processing time: 0.8015661239624023 seconds. Number of similar pairs: 1588\n",
      "Processing time: 0.10407495498657227 seconds. Number of similar pairs: 287\n",
      "Processing time: 1.1466290950775146 seconds. Number of similar pairs: 2\n",
      "Processing time: 0.06393885612487793 seconds. Number of similar pairs: 89\n",
      "Processing time: 0.06463217735290527 seconds. Number of similar pairs: 89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "dblp['year'] = dblp['year'].astype(str)\n",
    "acm['year'] = acm['year'].astype(str)\n",
    "\n",
    "\n",
    "selected_columns = ['author_names']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_a = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_p = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names','paper_title']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_ap = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names','publication_venue']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_apv = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names','year']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_ay = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_apy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names','publication_venue', 'year']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_apvy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'publication_venue', 'year']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_appvy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "\n",
    "selected_columns = ['paper_title','publication_venue']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_ppv = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')\n",
    "\n",
    "selected_columns = ['paper_title','publication_venue', 'year']\n",
    "dblp_l = vl.length_blocking_multi_columns_named(dblp, selected_columns)\n",
    "acm_l = vl.length_blocking_multi_columns_named(acm, selected_columns)\n",
    "length_ppvy = m.apply_similarity_lengths(dblp_l, acm_l, threshold, sim.exact_length_similarity, 'lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.014008068975403961, 'recall': 0.9985401459854014, 'f_measure': 0.027628549501151192}\n",
      "{'precision': 0.011381384375103845, 'recall': 1.0, 'f_measure': 0.022506612344137603}\n",
      "{'precision': 0.434010152284264, 'recall': 0.9985401459854014, 'f_measure': 0.6050420168067228}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.014008068975403961, 'recall': 0.9985401459854014, 'f_measure': 0.027628549501151192}\n",
      "{'precision': 0.5723849372384937, 'recall': 0.9985401459854014, 'f_measure': 0.727659574468085}\n",
      "{'precision': 0.434010152284264, 'recall': 0.9985401459854014, 'f_measure': 0.6050420168067228}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n",
      "{'precision': 0.0, 'recall': 0.0, 'f_measure': 0}\n"
     ]
    }
   ],
   "source": [
    "result_combined_length = (\n",
    "    evaluate_similarity(base_7_l, length_a) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_p) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_ap) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_apv) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_ay) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_appv) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_apy) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_appvy) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_ppv) +  \"\\n\" +\n",
    "    evaluate_similarity(base_7_l, length_ppvy) \n",
    ")\n",
    "\n",
    "print(result_combined_length)\n",
    "\n",
    "# only length_appv is okay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# return pairs in Format [1232, 2323]\n",
    "def read_matched_entities(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader) \n",
    "        matched_entities = [row for row in reader]\n",
    "    return matched_entities\n",
    "\n",
    "# print cluster based on the specific \n",
    "def print_clusters(clusters):\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f'Cluster {i + 1}: {cluster}')\n",
    "\n",
    "def cluster_to_csv(cluster_data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for idx, item in enumerate(cluster_data, start=1):\n",
    "            writer.writerow([f'Cluster {idx}: {item}'])\n",
    "\n",
    "# clustering\n",
    "file_path = '../Matched/Matched Entities.csv'\n",
    "matched_entities = read_matched_entities(file_path)\n",
    "cluster1 = c.build_clusters(matched_entities)\n",
    "cluster2 = c.clustering_matches(matched_entities)\n",
    "cluster_to_csv(cluster1, '../Matched/Clustered_Matched_Entities.csv' )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
